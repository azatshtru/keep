Probability is a real number between 0 and 1 that represents the how likely it is that the given event will happen.
# Events
Events are subsets of the set of all possible outcomes. For example, the set of all possible outcomes when rolling a die are {1, 2, 3, 4, 5, 6}. The event "roll an even number" is the subset {2, 4, 6}.

Each outcome is assigned a probability, for example, in the die, each outcome has equal probability 1/6. Then the event of rolling an even number can be calculated by summing over the probabilities in the event's subset { 2, 4, 6 }. Therefore, the probability is $1/6+1/6+1/6 = 1/2$

Similarly, the probabilities of subsets, intersections, unions and complements of events can be calculated.
- Compliment: $P(A') = 1 - P(A)$ where A is the subset associated with the event.
- Union: $P(A\cup B) = P(A) + P(B) - P(A\cap B)$
# Conditional probability
Conditional probability describes the probability of event A happening given that some event B has happened.
$$P(A|B) = \frac{P(A\cap B)}{P(B)}$$
Using conditional probability, another expression for the intersection of two events emerges.
$$P(A\cap B) = P(A)P(B|A) = P(B)P(A|B)$$
This can help us describe the probability of two events happening simultaneously.
# Independence
If two events are independent then probability of one happening doesn't affect the other, i.e. $P(A|B) = P(A)$ and $P(B|A) = P(B)$
This means that the probability of A and B happening at the same time is..
$P(A)P(B|A) = P(A)P(B)$
This effectively means that if two events are independent, the probability of both happening simultaneously is the product of the individual probabilities of both.
# Two methods to calculate probability
1. counting event occurrences using combinatorics and dividing them by total outcomes
2. calculating by simulating the outcome process

As an example, we calculate the probability of drawing three cards of the same number from a deck of 52 cards using combinatorics:
- Using combinatorics, there are $52 \choose 3$ ways of choosing 3 cards from a deck of 52 cards in total.
- there are $4 \choose 3$ ways to choose 3 cards of the same number from 4 suites, and there being 13 such cards in each suite, the ways of choosing the same number three times is $13{4\choose 3}$
- therefore, using combinatorics, the probability is ${13{4\choose 3}}/{52\choose 3}$

We can calculate the same by simulation..
- We have to choose 3 cards, there are 3 empty spaces _ _ _
- We can choose any card in the first space, but the card we choose will restrict the cards we can choose for the second place.
- If we choose any card in the first place, then there are only 3 cards remaining in the deck of that same number as the first place. Therefore, the probability of choosing the second card successfully will be 3/52.
- Again after choosing the 2nd card, the choices for the third card are limited, since there are only two cards left in the deck with the same number as the first and second place, hence the probability of choosing correctly will be 2/52
- The probability that the entire process succeeds is $1 \cdot {3}/{51} \cdot {2}/{52}$
Both the methods give the same probability of $1/425$

# Random variables
Random variable is a value generated by a random process. For example, the number of heads when tossing a coin some k times.
$P(X = x)$ is denoted as the probability of observing the value $x$ of a random variable $X$ when performing some random experiment.
# Expected value
Expected value is the average outcome of all outcomes in a probability model, i.e. average of all the values of a random variable.
$$E[X]=P(X=x)x$$

Expected value holds the linearity property, i.e. $E[aX_1 + bX_2] = aE[X_1] + bE[X_2]$
## boxes and balls :)
If there are n boxes and n balls, and the problem is to derive the expression for probability such that a box is empty. Also derive expected value of empty boxes using linearity.

If there are n balls and n boxes, each ball can choose to go into any of the n boxes creating n states, and the next ball can again go into any of the n boxes creating n more unique states from the previous n states, hence the ways n balls can go into n boxes is $n^n$.

If one box is empty, then n balls can go into n-1 boxes, since each ball can choose n-1 boxes and there are n balls, therefore there are $(n-1)^n$ unique states.
Probability that one box is empty is ${(n-1)^n}/{n^n}$
Expected value for number of empty boxes is $(n{(n-1)^n})/{n^n}$!!

# Random distribution
Random distribution is a table of probabilities for different values of the random variable. For example, if we take the random variable as "the sum of two dice", then the distribution of the probabilities of the random variable values would look like..
$$\begin{matrix}x&\bigg|&2&3&4&5&6&7&8&9&10&11&12\\P(X= x)&\bigg|&1/36&2/36&3/36&4/36&5/36&6/36&5/36&4/36&3/36&2/36&1/36\end{matrix}$$
## Uniform distribution
In a uniform distribution, the random variable X has n possible values $a,a+1,...,b$ and the probability of each value is $1/n$.
Expectation of uniform distribution is $E[X] = (a+b)/2$
## Binomial distribution
In a binomial distribution, the random variable X counts the number of successful attempts when $n$ attempts are made and the probability of a single success is $p$, and the probability that the random variable attains a value x, i.e. the probability of x successes out of n attempts is
$$P(X= x)= p^x(1− p)^{n−x} {n\choose x}$$
This comes from the fact that for exactly x successes, there should be n - x failures, so p is multiplied x times, and (1-p) is multiplied (n - x) times. The x success attempts can be chosen from any of the n total attempts, and there are ${n \choose x}$ ways to choose x success attempts from n total attempts.

For example, when throwing a dice ten times, the probability of throwing a six exactly three times is $(1/6)^3(5/6)^7 {10\choose 3}$
The expected value of X in a binomial distribution is
$$E[X]= pn$$
## Geometric distribution
In Geometric distribution, the probability of first success after n failures is measured. The random variable is "the number of attempts until success". If the probability of single success is p, then the distribution is..
$$P(X= x)= (1− p)^{x−1} p$$
The expected value of Geometric distribution is $E[X] = 1/p$

# Markov chains
Markov chains are represented by a set of states and valid actions one can take in that state and the probabilities of those actions.

For example, if there are n floors and there is a 1/2 probability that a person can either climb up or down, then this situation can be represented using Markov chains where the current floor is the state, up and down are the actions, and 1/2 and 1/2 respectively for up and down is the probability of those actions.

The state along with the probability of actions can be encoded into a vector. For example, if the person is on floor 2 and there are total 5 floors, then the Markov chain vector looks like $[1/2, 0, 1/2, 0, 0]$

# Two methods to simulate Markov chains
1. Dynamic programming can be used to memoize the Markov chain vectors at already visited states so that when they are visited again, their probability values can be accessed in O(1) time, then simulate markov chains starting from the current vector, and going through all possible transition states recursively.
2. Encoded matrices can also be used. When the vector is multiplied with the encoded matrix, it yields the probability distribution vector for the next state in the Markov chain.
# Randomized algorithms
Randomized algorithms can be used to speed up a lot of already existing algorithms and to find solutions to many algorithmic problems by injecting randomness in the algorithm's execution and utilising the probabilities of events that can happen during the execution.
1. **Monte Carlo** algorithms do not guarantee that the solution is correct but works very quickly. For these types of algorithms to work, the probability of failure should be very low.
2. **Las Vegas** algorithms are a bit slower than Monte Carlo algorithms, but they guarantee a solution.

## Order statistics and quickselect
Order statistic is kth largest or smallest element in an array. A simple way to calculate the kth order statistic is to just sort the array in O(nlogn) time. But a more efficient way in O(n) time is to use the quickselect algorithm.
In quickselect, a random element $x$ is selected from the array, then all the elements smaller than the $x$ are put before it and all the larger elements than $x$ are put after it. Let there be $a$ elements before $x$ and $b$ elements after $x$, then there can be three cases:
1. If $a=k$, then x is the $kth$ order statistic.
2. If $a<k$, then the $kth$ order statistic can be found in the second half by finding the $rth$ order statistic of the second half where $r=k-a$.
3. If $a>k$ then $kth$ order statistic is in the first half and can be recursively found.

quickselect belongs to the category of Las Vegas algorithms.
due to injecting randomness by choosing a random element, we divide the array in half each time we run the algorithm,
therefore in each execution we check half the elements of the previous execution:
$$n + n/2 + n/4 + n/8 < 2n = O(n)$$
Time complexity most of the time is $O(n)$, but it can happen, although highly improbable that we randomly choose the largest element in the array, in that case the array will not be halved, but will be split into two arrays of size n and n - 1, and in the worst possible case, if we always choose the largest element in each execution, then we will always process n - 1 elements than the previous execution.. 
$$\overbrace{n + (n - 1) + (n - 2) + \dots}^{n \text{ times}} < O(n^2)$$it can still (very improbably) reach $O(n^2)$ in the worst possible case.
## Frievald's algorithm
Explain Frievald's algorithm to verify matrix multiplication. Which category of random algorithm does it belong to?
Frievald's algorithm is a Monte Carlo algorithm that is used to verify matrix multiplication.
If there are three matrices $A, B, C$ such that we need to verify $AB=C$, then a one dimensional vector $X$ can be taken and $ABX=CX$ can be verified easily since $ABX$ can be associated like $A(BX)$, which requires only 2 one-dimensional vector multiplications and can be performed in $O(n^2)$, similarly $CX$ can be calculated in $O(n^2)$ as well.
This algorithm, however, can fail in some extremely rare cases although the probability is very low. To decrease the probability even further, we can multiply both sides by verifying using multiple random vectors $X$ before reporting the answer.
## Graph coloring
Given a graph that contains n nodes and m edges, our task is to find a way to color the nodes of the graph using two colors so that for at least m/2 edges, the endpoints have different colors.

The problem can be solved using a Las Vegas algorithm that generates random colorings until a valid coloring has been found. For each node, the chances of being colored either red or blue is 1/2. Due to this, for each edge, there is equal chance that its endpoints will have the same color or not.

Therefore there is 1/2 probability that the endpoints of an edge have different colors. The expected value of this probability for all m edges is m/2. Since it is expected that a random coloring is valid, we can find it.

